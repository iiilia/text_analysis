{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "all_right"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sbn\n",
    "\n",
    "import pickle as pkl\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import copy as cp\n",
    "import re\n",
    "import time\n",
    "import pymorphy2 as pm2\n",
    "\n",
    "import scipy as sc\n",
    "import sklearn.cross_validation  as cv\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import sklearn.metrics as metr\n",
    "import sklearn.ensemble as ens\n",
    "import sklearn.linear_model as lin\n",
    "import sklearn.feature_extraction as fe\n",
    "import sklearn.preprocessing as prep\n",
    "\n",
    "from functools import reduce\n",
    "import datetime as dt\n",
    "\n",
    "import numba as nb\n",
    "from numba import jit\n",
    "import sys\n",
    "sys.stderr.write('all_right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load train data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "\n",
    "# Load test data\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import multiprocessing as mltp\n",
    "\n",
    "thPool = ThreadPool(processes=mltp.cpu_count()*100)\n",
    "\n",
    "# Определяю функции для стемминга и лемматизации (стемминг использовался в самой первой итерации и показал себя хже лемматизации)\n",
    "\n",
    "import nltk\n",
    "from numba import jit,autojit\n",
    "\n",
    "st = nltk.stem.snowball.RussianStemmer()\n",
    "ma = pm2.MorphAnalyzer()\n",
    "\n",
    "# Обрабатываю текст объявления, убирая лишние символы и разделяя его на отдельные слова\n",
    "# Не все спецсимволы учтены\n",
    "# Также следовало бы учитывать \n",
    "#     - смайлики,\n",
    "#     - размеры (например, 99х99х99)\n",
    "#     - использование иностранных слов\n",
    "#     - использование цифр\n",
    "@jit\n",
    "def str_proc(x):\n",
    "    return x.lower().\\\n",
    "replace('. ',' ').replace(', ',' ').replace(': ',' ').replace('; ',' ').replace('! ',' ').replace('? ',' ').\\\n",
    "replace('.',' ').replace(',',' ').replace(':',' ').replace(';',' ').replace('!',' ').replace('?',' ').\\\n",
    "replace('\\n',' ').replace('\\t',' ').replace('(',' ').replace(')',' ').replace('«',' ').replace('»',' ').replace('\"',' ').\\\n",
    "replace('\\xa0',' ').replace('#',' ').replace('\\\\',' ').replace('/',' ').replace('|',' ').replace('*','').\\\n",
    "replace('{',' ').replace('}',' ').replace('[',' ').replace(']',' ').replace('+',' ').replace('&',' ').\\\n",
    "replace('@',' ').replace('№',' ').replace('%',' ').replace('^',' ').replace('<',' ').replace('>',' ').\\\n",
    "split(' ')\n",
    "\n",
    "def process_string_for_stemming_p(string):\n",
    "    return list(filter(lambda x: x != '', list(pool.map(lambda x: ma.parse(x)[0].normal_form , str_proc(string)))))\n",
    "\n",
    "def process_string_for_stemming(string):\n",
    "    return list(filter(lambda x: x != '', list(map(lambda x: ma.parse(x)[0].normal_form, str_proc(string)))))\n",
    "\n",
    "train['stem_description'] = list(thPool.map(str_proc, tqdm(train.description),chunksize=3000))\n",
    "train['stem_title'] = list(thPool.map(str_proc, tqdm(train.title),chunksize=3000))\n",
    "test['stem_description'] = list(thPool.map(str_proc, tqdm(test.description),chunksize=3000))\n",
    "test['stem_title'] = list(thPool.map(str_proc, tqdm(test.title),chunksize=3000))\n",
    "\n",
    "train['stem'] = list(map(lambda x,y: x+y,train.stem_title, train.stem_description))\n",
    "test['stem'] = list(map(lambda x,y: x+y,test.stem_title, test.stem_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Make 'word-count' feature\n",
    "\n",
    "listmerge5=lambda ll: [el for lst in tqdm(ll) for el in lst]\n",
    "\n",
    "my_stem = list(set(listmerge5(list(train.stem)+list(test.stem))))\n",
    "\n",
    "my_stem_all = listmerge5(list(train.stem)+list(test.stem))\n",
    "\n",
    "my_stem_vc = pd.Series(my_stem_all).value_counts()\n",
    "\n",
    "d = {i:j for i, j in zip(list(set(my_stem)),\\\n",
    "                 list(thPool.map(lambda x:ma.parse(x)[0].normal_form, tqdm(list(set(my_stem))), chunksize= 10000)))}\n",
    "\n",
    "d_df = pd.DataFrame()\n",
    "\n",
    "keys_ = [i for i in d.keys()]\n",
    "values_ = [d[i] for i in d.keys()]\n",
    "\n",
    "d_df['keys_'] = keys_\n",
    "d_df['values_'] = values_\n",
    "d_df['1'] = 1\n",
    "\n",
    "d_df_filtred = d_df[d_df['keys_'].isin(my_stem_vc[(my_stem_vc >= 5) == True].index) &\\\n",
    "     d_df['values_'].isin(d_df.groupby('values_')['1'].count()[d_df.groupby('values_')['1'].count() > 1].keys())][['values_','keys_']]\n",
    "\n",
    "d_df_filtred = d_df_filtred.set_index('keys_')\n",
    "\n",
    "# Выбираю для каждого слова в объявлении лемму, если оно попалов в список d_df_filtred\n",
    "def import_all_values(lst):\n",
    "    try:\n",
    "        return list(d_df_filtred.values_[lst])\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "train['stem_F'] = list(map(lambda x: import_all_values(x), tqdm(train.stem)))\n",
    "test['stem_F'] = list(map(lambda x: import_all_values(x), tqdm(test.stem)))\n",
    "\n",
    "# Фильтрую от NaNов и пустых слов\n",
    "@jit\n",
    "def filt(x):\n",
    "    for i in x:\n",
    "        if isinstance(i,float):\n",
    "            x.remove(i)\n",
    "        if i == '':\n",
    "            x.remove(i)\n",
    "    return x\n",
    "\n",
    "train['stem_F'] = list(map(lambda x: filt(x), tqdm(train.stem_F)))\n",
    "test['stem_F'] = list(map(lambda x: filt(x), tqdm(test.stem_F)))\n",
    "\n",
    "train['stem_bag'] = list(map(lambda x: ' '.join(list(map(str,x))), tqdm(train.stem_F)))\n",
    "test['stem_bag'] = list(map(lambda x: ' '.join(list(map(str,x))), tqdm(test.stem_F)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_pickle('train_without_stop_26_09.pkl')\n",
    "test.to_pickle('test_without_stop_26_09.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_pickle('train_without_stop_24_09.pkl')\n",
    "test = pd.read_pickle('test_without_stop_24_09.pkl')\n",
    "cat = pd.read_csv('data/category.csv')\n",
    "def proc_cat(cat):\n",
    "    l = cat.split('|')\n",
    "    while len(l) != 4:\n",
    "        l.append(l[len(l)-1])\n",
    "    return l\n",
    "for i in [1,2,3,4]:\n",
    "    cat[i] = 0\n",
    "cat.loc[:,[1,2,3,4]] = list(map(lambda x: proc_cat(x), cat.name))\n",
    "\n",
    "def categorise(series):\n",
    "    j = 0\n",
    "    res = series.copy()\n",
    "    for i in set(series):\n",
    "        res[series == i] = j\n",
    "        j = j + 1\n",
    "    return res\n",
    "\n",
    "for i in [1,2,3,4]:\n",
    "    cat[str(i)+'_cat'] = categorise(cat[i])\n",
    "cat = cat.set_index('category_id')\n",
    "\n",
    "d_cats = {}\n",
    "for x in cat.index:\n",
    "    d_cats.update([(x,{'1_cat':cat.loc[x,'1_cat'],'2_cat':cat.loc[x,'2_cat'],'3_cat':cat.loc[x,'3_cat'],\\\n",
    "                       '4_cat':cat.loc[x,'4_cat']})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# смотрю, есть ли у меня сохраненная структура с моделями и их скорами\n",
    "try:\n",
    "    f = open('my_current_models.pkl','rb')\n",
    "    models = pkl.load(f)\n",
    "    f.close()\n",
    "except:\n",
    "    cv_ = cv.StratifiedKFold(train.category_id, n_folds=5)\n",
    "    models = {'cross_val':cv_}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop-words by yandex list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = ['а','бы','быть','в','весь','вот','все','всей','вы','говорить','да','для','до','еще','же','знать','и','из','к',\\\n",
    "              'как','который','мочь','мы','на','наш','не','него','нее','нет','них','но','о','один','она','они','оно','оный',\\\n",
    "              'от','ото','по','с','свой','себя','сказать','та','такой','только','тот','ты','у','что','это','этот','я','','nan',\n",
    "              'рубль']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 489517/489517 [01:42<00:00, 4774.36it/s]\n",
      "100%|██████████| 243166/243166 [01:03<00:00, 3834.06it/s]\n"
     ]
    }
   ],
   "source": [
    "def to_str(l):\n",
    "    res = list(map(str,l))\n",
    "    res_2 = []\n",
    "    for j in res:\n",
    "        numbers = re_num.findall(j)\n",
    "        if len(numbers) != 0:\n",
    "            for numb in numbers:\n",
    "                j = j.replace(numb,'')\n",
    "                flag = True\n",
    "                for i in stop_words:\n",
    "                    if (i == numb) | (numb == ''):\n",
    "                        flag = False\n",
    "                if flag:\n",
    "                    res_2.append(numb)\n",
    "            flag = True\n",
    "            for i in stop_words:\n",
    "                if (i == j) | (j == ''):\n",
    "                    flag = False\n",
    "            if flag:\n",
    "                res_2.append(j)\n",
    "        else:\n",
    "            flag = True\n",
    "            for i in stop_words:\n",
    "                if (i == j) | (j == ''):\n",
    "                    flag = False\n",
    "            if flag:\n",
    "                res_2.append(j)\n",
    "    return res_2\n",
    "\n",
    "re_num = re.compile('\\d{1,10}')\n",
    "train['stem_F_str'] = list(map(to_str, tqdm(train.stem_F)))\n",
    "test['stem_F_str'] = list(map(to_str, tqdm(test.stem_F)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 489517/489517 [00:01<00:00, 435754.75it/s]\n",
      "100%|██████████| 243166/243166 [00:00<00:00, 439266.45it/s]\n"
     ]
    }
   ],
   "source": [
    "train['stem_F_str_for_BOW'] = list(map(lambda x: ' '.join(x), tqdm(train.stem_F_str)))\n",
    "test['stem_F_str_for_BOW'] = list(map(lambda x: ' '.join(x), tqdm(test.stem_F_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train[['item_id', 'title', 'description', 'price', 'category_id', 'stem_description', 'stem_title', 'stem',\n",
    "       'stem_F', 'stem_bag', 'stem_F_str', 'stem_F_str_for_BOW']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.columns = ['item_id', 'title', 'description', 'price', 'category_id', 'words_sep_descr', 'words_sep_title', 'all_words',\n",
    "       'all_lemma', 'glue_lemma', 'filtred_lemma', 'filtred_lemma_glue']\n",
    "test.columns = ['item_id', 'title', 'description', 'price', 'words_sep_descr', 'words_sep_title', 'all_words',\n",
    "       'all_lemma', 'glue_lemma', 'filtred_lemma', 'filtred_lemma_glue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.to_pickle('train_without_stop_26_09.pkl')\n",
    "test.to_pickle('test_without_stop_26_09.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute feature\n",
    "### Bag of words\n",
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Выделим из нашего корпуса мешок слов и tf-idf для каждого документа (то есть объявления))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "X_bag = vectorizer.fit_transform(list(train['filtred_lemma_glue'])+ list(test['filtred_lemma_glue']))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "X_tfidf = transformer.fit_transform(X_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tfidf_tr = X_tfidf[:train.shape[0],:]\n",
    "X_tfidf_te = X_tfidf[train.shape[0]:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_bag_tr = X_bag[:train.shape[0],:]\n",
    "X_bag_te = X_bag[train.shape[0]:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition\n",
    "### SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Так как деревья плохо работают со спаровыми матрицами, то я разложу нашу матрицу tf-idf для уменьшения размерности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.decomposition as dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = dec.TruncatedSVD(n_components=100)\n",
    "\n",
    "X_svd = svd.fit_transform(X_tfidf)\n",
    "\n",
    "X_svd_tr = X_svd[:train.shape[0],:]\n",
    "X_svd_te = X_svd[train.shape[0]:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd_10 = dec.TruncatedSVD(n_components=10)\n",
    "\n",
    "X_svd_10 = svd_10.fit_transform(X_tfidf)\n",
    "\n",
    "X_svd_10_tr = X_svd_10[:train.shape[0],:]\n",
    "X_svd_10_te = X_svd_10[train.shape[0]:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">По-хорошему, количестов компонентов разложения можно подобрать с помощью кросс-валидации\n",
    "\n",
    ">Равно как и параметры разложения (или даже его тип)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# обновление структуры с моделью и результатами \n",
    "def add_models(models,model,results,name = 'my_current_models.pkl'):\n",
    "    models.update([(str(dt.datetime.today()),{'model':cp.deepcopy(model),'result':cp.deepcopy(results)})])\n",
    "    f = open(name,'wb')\n",
    "    pkl.dump(models,f)\n",
    "    f.close()\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# рассчет точности по уровням категорий\n",
    "def accuracy_level(true,predict):\n",
    "    true_1 = list(map(lambda x: d_cats[x]['1_cat'],true))\n",
    "    predict_1 = list(map(lambda x: d_cats[x]['1_cat'],predict))\n",
    "    true_2 = list(map(lambda x: d_cats[x]['2_cat'],true))\n",
    "    predict_2 = list(map(lambda x: d_cats[x]['2_cat'],predict))\n",
    "    true_3 = list(map(lambda x: d_cats[x]['3_cat'],true))\n",
    "    predict_3 = list(map(lambda x: d_cats[x]['3_cat'],predict))\n",
    "    true_4 = list(map(lambda x: d_cats[x]['4_cat'],true))\n",
    "    predict_4 = list(map(lambda x: d_cats[x]['4_cat'],predict))\n",
    "    return [metr.accuracy_score(true_1,predict_1),\n",
    "           metr.accuracy_score(true_2,predict_2),\n",
    "           metr.accuracy_score(true_3,predict_3),\n",
    "           metr.accuracy_score(true_4,predict_4)]\n",
    "\n",
    "def accuracy_scoring_levels(est,X_true,predict):\n",
    "    return accuracy_level(est.predict(X_true),predict)    \n",
    "\n",
    "# подсчет скора по уровням категорий на кросс-валидации, запись в структуру\n",
    "def result_for_avito(cv_,true,pred,type_of_fea):\n",
    "    res = []\n",
    "    for i,j in cv_:\n",
    "        res.append(accuracy_level(true[j],pred[j]))\n",
    "    res = np.array(res)\n",
    "    d = {str(i+1)+'_level': {'score':j[1],'var':j[0]} for i,j in enumerate(zip(res.var(axis = 0),res.mean(axis = 0)))}\n",
    "    d.update([('type_of_fea',type_of_fea)])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Выводит топ моделей\n",
    "def print_top_models(models, n = 5, lvls = True, lvl = 4):\n",
    "    for i in models:\n",
    "        if len(models[i]) == 3:\n",
    "            del models[i]\n",
    "            break\n",
    "\n",
    "    r = []\n",
    "    for i in models:\n",
    "        if i != 'cross_val':\n",
    "            try:\n",
    "                if lvls == True:\n",
    "                    r.append([models[i]['model'].get_params(),models[i]['result']['4_level']['score'],\\\n",
    "                              models[i]['result']['3_level']['score'],\\\n",
    "                              models[i]['result']['2_level']['score'],models[i]['result']['1_level']['score']])\n",
    "                else:\n",
    "                    r.append([models[i]['model'].get_params(),models[i]['result'][str(lvl)+'_level']['score']])\n",
    "            except:\n",
    "                print(i,' ',models[i])\n",
    "\n",
    "    df = pd.DataFrame(r).sort(1,ascending = False).head(n)\n",
    "    if lvls == True:\n",
    "        df.columns = ['Model','Accuracy for 4 level','Accuracy for 3 level','Accuracy for 2 level','Accuracy for 1 level']\n",
    "    else:\n",
    "        df.columns = ['Model','Accuracy for '+str(lvl)+' level']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CV-folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We define cv-folds on point of loading 'my_current_models.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lin\n",
    "import sklearn.ensemble as ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Next!"
     ]
    }
   ],
   "source": [
    "model = lin.LogisticRegression(fit_intercept=True,n_jobs=-1)\n",
    "res = result_for_avito(models['cross_val'],train.category_id, cv.cross_val_predict(model, X_tfidf_tr,train.category_id,n_jobs=-1,\\\n",
    "                                    cv = models['cross_val']),'tf-idf_without_stopwords')\n",
    "models = add_models(models,model,res)\n",
    "\n",
    "model = lin.LogisticRegression(fit_intercept=False,n_jobs=-1)\n",
    "res = result_for_avito(models['cross_val'],train.category_id, cv.cross_val_predict(model, X_tfidf_tr,train.category_id,n_jobs=-1,\\\n",
    "                                    cv = models['cross_val']),'tf-idf_without_stopwords')\n",
    "models = add_models(models,model,res)\n",
    "sys.stderr.write('Next!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [1:02:23<00:00, 1230.12s/it]\n",
      "Next!"
     ]
    }
   ],
   "source": [
    "for c_ in tqdm([1.5,2,2.5]):\n",
    "    LR = lin.LogisticRegression(fit_intercept=False,C = c_,n_jobs=-1)\n",
    "    res = result_for_avito(models['cross_val'],train.category_id, \\\n",
    "                           cv.cross_val_predict(LR, X_tfidf_tr,train.category_id,n_jobs=-1,\\\n",
    "                                        cv = models['cross_val']),'tf-idf_without_stopwords')\n",
    "    models = add_models(models,LR,res)\n",
    "    \n",
    "    LR = lin.LogisticRegression(fit_intercept=True,C = c_,n_jobs=-1)\n",
    "    res = result_for_avito(models['cross_val'],train.category_id, \\\n",
    "                           cv.cross_val_predict(LR, X_tfidf_tr,train.category_id,n_jobs=-1,\\\n",
    "                                        cv = models['cross_val']),'tf-idf_without_stopwords')\n",
    "    models = add_models(models,LR,res)\n",
    "sys.stderr.write('Next!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [2:01:13<00:00, 867.55s/it]\n",
      "Next!"
     ]
    }
   ],
   "source": [
    "for c_ in tqdm([3,3.5,4,5,5.5,6,7,8,10]):\n",
    "    LR = lin.LogisticRegression(fit_intercept=True,C = c_,n_jobs=-1)\n",
    "    res = result_for_avito(models['cross_val'],train.category_id,\\\n",
    "                           cv.cross_val_predict(LR, X_tfidf_tr,train.category_id,n_jobs=-1,\\\n",
    "                                        cv = models['cross_val']),'tf-idf_without_stopwords')\n",
    "    models = add_models(models,LR,res)\n",
    "sys.stderr.write('Next!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Можно увидеть, что логистическая регрессия на заданных параметрах с регулируемыми параметрами регуляризации и fit_intercept достигает максимума по сетке при C = 4.0 и fit_intercept = True.\n",
    "\n",
    "> При этом в топ-8 результатов нет моделей без свободного члена.\n",
    "\n",
    "> Попробуем посмотреть в окрестности C = 4, а именно на сетке [3.7,4.2,4.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [36:41<00:00, 730.23s/it]\n",
      "Next!"
     ]
    }
   ],
   "source": [
    "for c_ in tqdm([3.7,4.2,4.5]):\n",
    "    LR = lin.LogisticRegression(fit_intercept=True,C = c_,n_jobs=-1)\n",
    "    res = result_for_avito(models['cross_val'],train.category_id,\\\n",
    "                           cv.cross_val_predict(LR, X_tfidf_tr,train.category_id,n_jobs=-1,\\\n",
    "                                        cv = models['cross_val']),'tf-idf_without_stopwords')\n",
    "    models = add_models(models,LR,res)\n",
    "sys.stderr.write('Next!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> И на чуть более мелкой сетке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [39:15<00:00, 783.25s/it]\n",
      "Next!"
     ]
    }
   ],
   "source": [
    "for c_ in tqdm([3.8,3.9,4.1]):\n",
    "    LR = lin.LogisticRegression(fit_intercept=True,C = c_,n_jobs=-1)\n",
    "    res = result_for_avito(models['cross_val'],train.category_id,\\\n",
    "                           cv.cross_val_predict(LR, X_tfidf_tr,train.category_id,n_jobs=-1,\\\n",
    "                                        cv = models['cross_val']),'tf-idf_without_stopwords')\n",
    "    models = add_models(models,LR,res)\n",
    "sys.stderr.write('Next!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Найдем среди моделей LR самые лучшие и посмотрим их скор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.842087163982 \t 0.952763528875 \t 3.9 \t True \t tf-idf_without_stopwords\n",
      "0.842078991586 \t 0.952790091408 \t 4 \t True \t tf-idf_without_stopwords\n",
      "0.842068777894 \t 0.952755353494 \t 3.8 \t True \t tf-idf_without_stopwords\n",
      "0.842052442804 \t 0.952769671809 \t 4.1 \t True \t tf-idf_without_stopwords\n",
      "0.842044273036 \t 0.952775800303 \t 4.2 \t True \t tf-idf_without_stopwords\n",
      "0.842036094253 \t 0.952734928241 \t 3.7 \t True \t tf-idf_without_stopwords\n",
      "0.841989104524 \t 0.952632777766 \t 3.5 \t True \t tf-idf_without_stopwords\n",
      "0.841982987009 \t 0.952859557887 \t 4.5 \t True \t tf-idf_without_stopwords\n",
      "0.841901266957 \t 0.95247344069 \t 3 \t True \t tf-idf_without_stopwords\n",
      "0.841866538459 \t 0.952896327515 \t 5 \t True \t tf-idf_without_stopwords\n",
      "0.841862451236 \t 0.952961700722 \t 5.5 \t True \t tf-idf_without_stopwords\n",
      "0.841705167165 \t 0.95231820246 \t 3.9 \t True \t tf-idf_without_stopwords\n",
      "0.841639782478 \t 0.952906540205 \t 6 \t True \t tf-idf_without_stopwords\n",
      "0.841580558609 \t 0.95223034693 \t 2.5 \t True \t tf-idf_without_stopwords\n",
      "0.841286375653 \t 0.952861608632 \t 7 \t True \t tf-idf_without_stopwords\n",
      "0.841045314947 \t 0.951744156864 \t 2 \t True \t tf-idf_without_stopwords\n",
      "0.840898243332 \t 0.952624640613 \t 8 \t True \t tf-idf_without_stopwords\n",
      "0.840175069411 \t 0.95238564238 \t 10 \t True \t tf-idf_without_stopwords\n",
      "0.840046384464 \t 0.95093112601 \t 1.5 \t True \t tf-idf_without_stopwords\n",
      "0.837801303749 \t 0.949435778269 \t 1.0 \t True \t tf-idf_without_stopwords\n",
      "0.837776809909 \t 0.950361182469 \t 2.5 \t False \t tf-idf_without_stopwords\n",
      "0.836702268846 \t 0.949633930941 \t 2 \t False \t tf-idf_without_stopwords\n",
      "0.834837184257 \t 0.948365331455 \t 1.5 \t False \t tf-idf_without_stopwords\n",
      "0.831652469664 \t 0.946318457143 \t 1.0 \t False \t tf-idf_without_stopwords\n"
     ]
    }
   ],
   "source": [
    "for i in sorted({i:models[i] for i in {k:models[k] for k in models if (k != 'cross_val')} \\\n",
    "               if (type(models[i]['model']) == type(lin.LogisticRegression()) )},\\\n",
    "              key = lambda x: models[x]['result']['4_level']['score'],reverse=True):\n",
    "    print(models[i]['result']['4_level']['score'],'\\t',models[i]['result']['1_level']['score'],'\\t',models[i]['model'].get_params()['C'],'\\t',\\\n",
    "          models[i]['model'].get_params()['fit_intercept'],'\\t',models[i]['result']['type_of_fea'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Лучшая логичтическая регрессия имеет коэф регуляризации 3.9 и обучается вместе со свободным членом\n",
    "\n",
    "> Попробуем увеличить число итераций и понизить tolerance (критерий останова)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 22.5min finished\n",
      "Next!"
     ]
    }
   ],
   "source": [
    "LR = lin.LogisticRegression(fit_intercept=True,C = 3.9,n_jobs=-1,tol=0.000001,max_iter = 1000)\n",
    "res = result_for_avito(models['cross_val'],train.category_id,\\\n",
    "                           cv.cross_val_predict(LR, X_tfidf_tr,train.category_id,n_jobs=-1,\\\n",
    "                                        cv = models['cross_val'],verbose = 2),'tf-idf_without_stopwords')\n",
    "models = add_models(models,LR,res)\n",
    "sys.stderr.write('Next!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1_level': {'score': 0.95231820245959287, 'var': 4.8217750614520183e-07},\n",
       " '2_level': {'score': 0.92384119332607251, 'var': 1.8064638108986135e-07},\n",
       " '3_level': {'score': 0.84622798700725466, 'var': 1.9709426299616991e-07},\n",
       " '4_level': {'score': 0.84170516716535171, 'var': 1.3020843780604523e-07},\n",
       " 'type_of_fea': 'tf-idf_without_stopwords'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Как-то неожиданно увеличение числа итераций подбора параметров ухудшило скор (вероятно, это произошло из за низкого толеранса, при котором модель переобучилась)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попробуем случайный лес на SVD (100) для tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for i in tqdm([10,25,50,75,100,150,200,300]):\n",
    "    RF = ens.RandomForestClassifier(n_estimators=i,max_depth=13,n_jobs=-1)\n",
    "    res = result_for_avito(models['cross_val'],train.category_id, \\\n",
    "                               cv.cross_val_predict(RF, X_svd_tr,train.category_id,n_jobs=1,\\\n",
    "                                            cv = models['cross_val']),'svd_100_tfidf')\n",
    "    models = add_models(models,RF,res)\n",
    "sys.stderr.write('Next!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.3min finished\n",
      " 10%|█         | 1/10 [05:21<48:13, 321.46s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.3min finished\n",
      " 20%|██        | 2/10 [10:43<42:53, 321.67s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.1min finished\n",
      " 30%|███       | 3/10 [15:53<37:06, 318.05s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  4.8min finished\n",
      " 40%|████      | 4/10 [20:44<31:00, 310.01s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  4.4min finished\n",
      " 50%|█████     | 5/10 [25:12<24:47, 297.51s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.9min finished\n",
      " 60%|██████    | 6/10 [29:10<18:38, 279.66s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.8min finished\n",
      " 70%|███████   | 7/10 [33:01<13:15, 265.00s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.6min finished\n",
      " 80%|████████  | 8/10 [36:40<08:22, 251.11s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.2min finished\n",
      " 90%|█████████ | 9/10 [39:52<03:53, 233.34s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.8min finished\n",
      "100%|██████████| 10/10 [42:39<00:00, 213.46s/it]\n",
      "Next!"
     ]
    }
   ],
   "source": [
    "max_depths = [15,18,21,23,24,29,35,40,43,50]\n",
    "max_depths.reverse()\n",
    "for i in tqdm(max_depths):\n",
    "    RF = ens.RandomForestClassifier(n_estimators=75,max_depth=i,n_jobs=-1)\n",
    "    res = result_for_avito(models['cross_val'],train.category_id, \\\n",
    "                               cv.cross_val_predict(RF, X_svd_tr,train.category_id,n_jobs=1,\\\n",
    "                                            cv = models['cross_val'],verbose = 2),'svd_100_tfidf')\n",
    "    models = add_models(models,RF,res)\n",
    "sys.stderr.write('Next!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попробуем такие же деревья, но с Ценой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [28:57<00:00, 313.56s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.3min finished\n",
      " 10%|█         | 1/10 [05:18<47:50, 318.95s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.2min finished\n",
      " 20%|██        | 2/10 [10:33<42:21, 317.68s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.1min finished\n",
      " 30%|███       | 3/10 [15:39<36:39, 314.19s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  4.9min finished\n",
      " 40%|████      | 4/10 [20:34<30:50, 308.41s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  4.4min finished\n",
      " 50%|█████     | 5/10 [25:03<24:42, 296.53s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  4.0min finished\n",
      " 60%|██████    | 6/10 [29:04<18:40, 280.01s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.8min finished\n",
      " 70%|███████   | 7/10 [32:52<13:12, 264.21s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.6min finished\n",
      " 80%|████████  | 8/10 [36:27<08:19, 249.52s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.1min finished\n",
      " 90%|█████████ | 9/10 [39:38<03:51, 231.97s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.7min finished\n",
      "100%|██████████| 10/10 [42:24<00:00, 212.14s/it]\n",
      "Next!"
     ]
    }
   ],
   "source": [
    "for i in tqdm([10,25,50,75,100,150,200,300]):\n",
    "    RF = ens.RandomForestClassifier(n_estimators=i,max_depth=13,n_jobs=-1)\n",
    "    res = result_for_avito(models['cross_val'],train.category_id, \\\n",
    "                               cv.cross_val_predict(RF, np.hstack((X_svd_tr,np.array(train.price).reshape([train.shape[0],1])))\\\n",
    "                                                    ,train.category_id,n_jobs=1,\\\n",
    "                                            cv = models['cross_val']),'svd_100_tfidf+price')\n",
    "    models = add_models(models,RF,res)\n",
    "    \n",
    "for i in tqdm(max_depths):\n",
    "    RF = ens.RandomForestClassifier(n_estimators=75,max_depth=i,n_jobs=-1)\n",
    "    res = result_for_avito(models['cross_val'],train.category_id, \\\n",
    "                               cv.cross_val_predict(RF, np.hstack((X_svd_tr,np.array(train.price).reshape([train.shape[0],1])))\\\n",
    "                                                    ,train.category_id,n_jobs=1,\\\n",
    "                                            cv = models['cross_val'],verbose = 2),'svd_100_tfidf+price')\n",
    "    models = add_models(models,RF,res)\n",
    "sys.stderr.write('Next!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пробуем на SVD (10) деревья + Цена"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [09:31<00:00, 101.97s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.3min finished\n",
      " 10%|█         | 1/10 [01:19<11:57, 79.76s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.3min finished\n",
      " 20%|██        | 2/10 [02:37<10:33, 79.17s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.3min finished\n",
      " 30%|███       | 3/10 [03:54<09:09, 78.54s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.3min finished\n",
      " 40%|████      | 4/10 [05:13<07:51, 78.59s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.2min finished\n",
      " 50%|█████     | 5/10 [06:29<06:29, 77.86s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.2min finished\n",
      " 60%|██████    | 6/10 [07:42<05:05, 76.32s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.1min finished\n",
      " 70%|███████   | 7/10 [08:51<03:42, 74.33s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.1min finished\n",
      " 80%|████████  | 8/10 [09:58<02:24, 72.01s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   58.2s finished\n",
      " 90%|█████████ | 9/10 [10:58<01:08, 68.48s/it][Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   51.7s finished\n",
      "100%|██████████| 10/10 [11:52<00:00, 64.05s/it]\n",
      "Next!"
     ]
    }
   ],
   "source": [
    "for i in tqdm([10,25,50,75,100,150,200,300]):\n",
    "    RF = ens.RandomForestClassifier(n_estimators=i,max_depth=13,n_jobs=-1)\n",
    "    res = result_for_avito(models['cross_val'],train.category_id, \\\n",
    "                               cv.cross_val_predict(RF, np.hstack((X_svd_10_tr,np.array(train.price).reshape([train.shape[0],1])))\\\n",
    "                                                    ,train.category_id,n_jobs=1,\\\n",
    "                                            cv = models['cross_val']),'svd_10_tfidf+price')\n",
    "    models = add_models(models,RF,res)\n",
    "    \n",
    "for i in tqdm(max_depths):\n",
    "    RF = ens.RandomForestClassifier(n_estimators=75,max_depth=i,n_jobs=-1)\n",
    "    res = result_for_avito(models['cross_val'],train.category_id, \\\n",
    "                               cv.cross_val_predict(RF, np.hstack((X_svd_10_tr,np.array(train.price).reshape([train.shape[0],1])))\\\n",
    "                                                    ,train.category_id,n_jobs=1,\\\n",
    "                                            cv = models['cross_val'],verbose = 2),'svd_10_tfidf+price')\n",
    "    models = add_models(models,RF,res)\n",
    "sys.stderr.write('Next!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выцепим из RF-моделей самую лучшую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.690302962694 \t 75 \t 50 \t svd_100_tfidf+price\n",
      "0.688887184306 \t 75 \t 43 \t svd_100_tfidf+price\n",
      "0.685931285749 \t 75 \t 40 \t svd_100_tfidf+price\n",
      "0.680454450536 \t 75 \t 50 \t svd_100_tfidf\n",
      "0.67870170981 \t 75 \t 43 \t svd_100_tfidf\n",
      "0.677710784862 \t 75 \t 35 \t svd_100_tfidf+price\n",
      "0.676082776803 \t 75 \t 40 \t svd_100_tfidf\n",
      "0.666033904251 \t 75 \t 35 \t svd_100_tfidf\n",
      "0.657685057257 \t 75 \t 29 \t svd_100_tfidf+price\n",
      "0.645591566656 \t 75 \t 29 \t svd_100_tfidf\n",
      "0.630846015062 \t 75 \t 24 \t svd_100_tfidf+price\n",
      "0.62488348971 \t 75 \t 23 \t svd_100_tfidf+price\n",
      "0.618487159763 \t 75 \t 24 \t svd_100_tfidf\n",
      "0.612440751541 \t 75 \t 21 \t svd_100_tfidf+price\n",
      "0.610733040905 \t 75 \t 23 \t svd_100_tfidf\n",
      "0.599039172711 \t 75 \t 21 \t svd_100_tfidf\n",
      "0.582255830489 \t 75 \t 18 \t svd_100_tfidf+price\n",
      "0.568036966958 \t 75 \t 18 \t svd_100_tfidf\n",
      "0.549059348912 \t 75 \t 15 \t svd_100_tfidf+price\n",
      "0.532780804344 \t 75 \t 15 \t svd_100_tfidf\n",
      "0.532724695979 \t 200 \t 13 \t svd_100_tfidf+price\n",
      "0.531855027579 \t 300 \t 13 \t svd_100_tfidf+price\n",
      "0.530414297254 \t 150 \t 13 \t svd_100_tfidf+price\n",
      "0.526284246556 \t 75 \t 13 \t svd_100_tfidf+price\n",
      "0.523434015417 \t 100 \t 13 \t svd_100_tfidf+price\n",
      "0.518846276604 \t 50 \t 13 \t svd_100_tfidf+price\n",
      "0.517763322777 \t 300 \t 13 \t svd_100_tfidf\n",
      "0.517471497945 \t 300 \t 13 \t svd_100_tfidf\n",
      "0.516633943204 \t 200 \t 13 \t svd_100_tfidf\n",
      "0.515069217507 \t 150 \t 13 \t svd_100_tfidf\n",
      "0.514259938946 \t 200 \t 13 \t svd_100_tfidf\n",
      "0.51194179142 \t 100 \t 13 \t svd_100_tfidf\n",
      "0.511195400286 \t 150 \t 13 \t svd_100_tfidf\n",
      "0.507404611005 \t 75 \t 13 \t svd_100_tfidf\n",
      "0.505440230581 \t 25 \t 13 \t svd_100_tfidf+price\n",
      "0.505244742581 \t 100 \t 13 \t svd_100_tfidf\n",
      "0.503871740353 \t 75 \t 13 \t svd_100_tfidf\n",
      "0.499318953234 \t 50 \t 13 \t svd_100_tfidf\n",
      "0.497884319027 \t 50 \t 13 \t svd_100_tfidf\n",
      "0.481736123394 \t 25 \t 13 \t svd_100_tfidf\n",
      "0.48169491318 \t 25 \t 13 \t svd_100_tfidf\n",
      "0.479856776662 \t 75 \t 29 \t svd_10_tfidf+price\n",
      "0.479609556396 \t 75 \t 35 \t svd_10_tfidf+price\n",
      "0.479601498084 \t 75 \t 40 \t svd_10_tfidf+price\n",
      "0.479250138792 \t 75 \t 50 \t svd_10_tfidf+price\n",
      "0.479113241441 \t 75 \t 43 \t svd_10_tfidf+price\n",
      "0.47703359575 \t 75 \t 24 \t svd_10_tfidf+price\n",
      "0.476428588736 \t 10 \t 13 \t svd_100_tfidf+price\n",
      "0.475397332388 \t 75 \t 23 \t svd_10_tfidf+price\n",
      "0.469129847694 \t 75 \t 21 \t svd_10_tfidf+price\n",
      "0.451588115044 \t 75 \t 18 \t svd_10_tfidf+price\n",
      "0.449610128302 \t 10 \t 13 \t svd_100_tfidf\n",
      "0.444357550525 \t 10 \t 13 \t svd_100_tfidf\n",
      "0.424404107382 \t 75 \t 15 \t svd_10_tfidf+price\n",
      "0.403340377069 \t 200 \t 13 \t svd_10_tfidf+price\n",
      "0.402974789488 \t 300 \t 13 \t svd_10_tfidf+price\n",
      "0.402533575709 \t 150 \t 13 \t svd_10_tfidf+price\n",
      "0.402380239989 \t 100 \t 13 \t svd_10_tfidf+price\n",
      "0.401009631575 \t 75 \t 13 \t svd_10_tfidf+price\n",
      "0.400317183811 \t 50 \t 13 \t svd_10_tfidf+price\n",
      "0.39799042043 \t 25 \t 13 \t svd_10_tfidf+price\n",
      "0.384526065021 \t 10 \t 13 \t svd_10_tfidf+price\n"
     ]
    }
   ],
   "source": [
    "for i in sorted({i:models[i] for i in {k:models[k] for k in models if (k != 'cross_val')} \\\n",
    "               if (type(models[i]['model']) == type(ens.RandomForestClassifier()))},\\\n",
    "              key = lambda x: models[x]['result']['4_level']['score'],reverse=True)[:20]:\n",
    "    print(models[i]['result']['4_level']['score'],'\\t',models[i]['model'].get_params()['n_estimators'],'\\t',\\\n",
    "          models[i]['model'].get_params()['max_depth'],'\\t',models[i]['result']['type_of_fea'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построим более глубокие деревья (при числе деревьев в 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm([100,75,65,60,55,50]):\n",
    "    RF = ens.RandomForestClassifier(n_estimators=200,max_depth=i,n_jobs=-1)\n",
    "    res = result_for_avito(models['cross_val'],train.category_id, \\\n",
    "                               cv.cross_val_predict(RF, np.hstack((X_svd_tr,np.array(train.price).reshape([train.shape[0],1])))\\\n",
    "                                                    ,train.category_id,n_jobs=1,\\\n",
    "                                            cv = models['cross_val']),'svd_100_tfidf+price')\n",
    "    models = add_models(models,RF,res)\n",
    "sys.stderr.write('Next!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Посмотрим на то, какие модели Random Forest обеспечивают наилучший скор на кросс-валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.699906260572 \t 0.860352032277 \t 200 \t 55 \t svd_100_tfidf+price\n",
      "0.699846974883 \t 0.860523612408 \t 200 \t 60 \t svd_100_tfidf+price\n",
      "0.699802068939 \t 0.860157968938 \t 200 \t 75 \t svd_100_tfidf+price\n",
      "0.699785682364 \t 0.860466380515 \t 200 \t 65 \t svd_100_tfidf+price\n",
      "0.699667215839 \t 0.86048076054 \t 200 \t 100 \t svd_100_tfidf+price\n",
      "0.699501837404 \t 0.859880132825 \t 200 \t 50 \t svd_100_tfidf+price\n",
      "0.690302962694 \t 0.854681123439 \t 75 \t 50 \t svd_100_tfidf+price\n",
      "0.688887184306 \t 0.853339003735 \t 75 \t 43 \t svd_100_tfidf+price\n",
      "0.685931285749 \t 0.851475974198 \t 75 \t 40 \t svd_100_tfidf+price\n",
      "0.680454450536 \t 0.85192738949 \t 75 \t 50 \t svd_100_tfidf\n"
     ]
    }
   ],
   "source": [
    "for i in sorted({i:models[i] for i in {k:models[k] for k in models if (k != 'cross_val')} \\\n",
    "               if (type(models[i]['model']) == type(ens.RandomForestClassifier()))},\\\n",
    "              key = lambda x: models[x]['result']['4_level']['score'],reverse=True)[:10]:\n",
    "    print(models[i]['result']['4_level']['score'],'\\t',models[i]['result']['1_level']['score'],'\\t',models[i]['model'].get_params()['n_estimators'],'\\t',\\\n",
    "          models[i]['model'].get_params()['max_depth'],'\\t',models[i]['result']['type_of_fea'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Выберем Random Forest с 200 деревьями и с максимальной глубиной 55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [06:05<00:00, 44.73s/it]\n",
      "Next!"
     ]
    }
   ],
   "source": [
    "n_ests = [50,75,100,125,150,175,200]\n",
    "n_ests.reverse()\n",
    "for i in tqdm(n_ests):\n",
    "    ET = ens.ExtraTreesClassifier(n_estimators=i,max_depth=10,n_jobs=-1)\n",
    "    res = result_for_avito(models['cross_val'],train.category_id, \\\n",
    "                               cv.cross_val_predict(ET, np.hstack((X_svd_tr,np.array(train.price).reshape([train.shape[0],1])))\\\n",
    "                                                    ,train.category_id,n_jobs=1,\\\n",
    "                                            cv = models['cross_val']),'svd_100_tfidf+price')\n",
    "    models = add_models(models,ET,res)\n",
    "sys.stderr.write('Next!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.553835655191 \t 200 \t 10 \t svd_100_tfidf+price\n",
      "0.552021854436 \t 175 \t 10 \t svd_100_tfidf+price\n",
      "0.548802110721 \t 150 \t 10 \t svd_100_tfidf+price\n",
      "0.546083288366 \t 125 \t 10 \t svd_100_tfidf+price\n",
      "0.542403814246 \t 100 \t 10 \t svd_100_tfidf+price\n",
      "0.539760566573 \t 75 \t 10 \t svd_100_tfidf+price\n",
      "0.528024437864 \t 50 \t 10 \t svd_100_tfidf+price\n"
     ]
    }
   ],
   "source": [
    "for i in sorted({i:models[i] for i in {k:models[k] for k in models if (k != 'cross_val')} \\\n",
    "               if (type(models[i]['model']) == type(ens.ExtraTreesClassifier()))},\\\n",
    "              key = lambda x: models[x]['result']['4_level']['score'],reverse=True):\n",
    "    print(models[i]['result']['4_level']['score'],'\\t',models[i]['model'].get_params()['n_estimators'],'\\t',\\\n",
    "          models[i]['model'].get_params()['max_depth'],'\\t',models[i]['result']['type_of_fea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [16:50<00:00, 130.50s/it]\n",
      "Next!"
     ]
    }
   ],
   "source": [
    "md = [15,20,25,30,40,50,75]\n",
    "md.reverse()\n",
    "for i in tqdm(md):\n",
    "    ET = ens.ExtraTreesClassifier(n_estimators=200,max_depth=i,n_jobs=-1)\n",
    "    res = result_for_avito(models['cross_val'],train.category_id, \\\n",
    "                               cv.cross_val_predict(ET, np.hstack((X_svd_tr,np.array(train.price).reshape([train.shape[0],1])))\\\n",
    "                                                    ,train.category_id,n_jobs=1,\\\n",
    "                                            cv = models['cross_val']),'svd_100_tfidf+price')\n",
    "    models = add_models(models,ET,res)\n",
    "sys.stderr.write('Next!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.695371249325 \t 200 \t 75 \t svd_100_tfidf+price\n",
      "0.694676709922 \t 200 \t 50 \t svd_100_tfidf+price\n",
      "0.688764728199 \t 200 \t 40 \t svd_100_tfidf+price\n",
      "0.666610198248 \t 200 \t 30 \t svd_100_tfidf+price\n",
      "0.644858121232 \t 200 \t 25 \t svd_100_tfidf+price\n",
      "0.620340076667 \t 200 \t 20 \t svd_100_tfidf+price\n",
      "0.592267840018 \t 200 \t 15 \t svd_100_tfidf+price\n",
      "0.553835655191 \t 200 \t 10 \t svd_100_tfidf+price\n",
      "0.552021854436 \t 175 \t 10 \t svd_100_tfidf+price\n",
      "0.548802110721 \t 150 \t 10 \t svd_100_tfidf+price\n",
      "0.546083288366 \t 125 \t 10 \t svd_100_tfidf+price\n",
      "0.542403814246 \t 100 \t 10 \t svd_100_tfidf+price\n",
      "0.539760566573 \t 75 \t 10 \t svd_100_tfidf+price\n",
      "0.528024437864 \t 50 \t 10 \t svd_100_tfidf+price\n"
     ]
    }
   ],
   "source": [
    "for i in sorted({i:models[i] for i in {k:models[k] for k in models if (k != 'cross_val')} \\\n",
    "               if (type(models[i]['model']) == type(ens.ExtraTreesClassifier()))},\\\n",
    "              key = lambda x: models[x]['result']['4_level']['score'],reverse=True):\n",
    "    print(models[i]['result']['4_level']['score'],'\\t',models[i]['model'].get_params()['n_estimators'],'\\t',\\\n",
    "          models[i]['model'].get_params()['max_depth'],'\\t',models[i]['result']['type_of_fea'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Выберем ExtraTreesClassifier с 200 деревьями и глубиной 75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сливаний файлов с моделями в один"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('models2.pkl','wb')\n",
    "pkl.dump(models,f)\n",
    "f.close()\n",
    "\n",
    "f = open('/home/ivanitskiy/nas_storage/projects/houses/RF_25/folder3/models1.pkl','rb')\n",
    "models1 = pkl.load(f)\n",
    "f.close()\n",
    "f = open('models2.pkl','rb')\n",
    "models2 = pkl.load(f)\n",
    "f.close()\n",
    "\n",
    "for i in models2:\n",
    "    models1.update([(i,models2[i])])\n",
    "\n",
    "f = open('my_current_models','wb')\n",
    "pkl.dump(models1,f)\n",
    "f.close()\n",
    "\n",
    "f = open('my_current_models','rb')\n",
    "models = pkl.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стакнем результаты лучшей линейной модели и лучших деревьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_predict_proba(models,model,train,target):\n",
    "    res_proba = np.zeros([train.shape[0],target.nunique()])\n",
    "    for train_fold,test_fold in tqdm(models['cross_val']):\n",
    "        model.fit(train[train_fold,:],target[train_fold])\n",
    "        res_proba[test_fold,:] = model.predict_proba(train[test_fold,:])\n",
    "    return res_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [25:23<00:00, 304.76s/it]\n",
      "100%|██████████| 5/5 [13:06<00:00, 157.21s/it]\n",
      "100%|██████████| 5/5 [02:50<00:00, 34.08s/it]\n",
      "Next!"
     ]
    }
   ],
   "source": [
    "res_proba_LR = cv_predict_proba(models,lin.LogisticRegression(C=3.9,fit_intercept = True,\\\n",
    "                                max_iter = 100,n_jobs = -1),X_tfidf_tr,train.category_id)\n",
    "res_proba_RF = cv_predict_proba(models,ens.RandomForestClassifier(max_depth = 55, n_estimators=200,n_jobs = -1),\\\n",
    "                                np.hstack((X_svd_tr,np.array(train.price).reshape([train.shape[0],1])))  ,train.category_id)\n",
    "res_proba_ET = cv_predict_proba(models,ens.ExtraTreesClassifier(n_estimators=200,max_depth=75,n_jobs=-1),\\\n",
    "                                np.hstack((X_svd_tr,np.array(train.price).reshape([train.shape[0],1])))  ,train.category_id)\n",
    "sys.stderr.write('Next!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_data = np.hstack((res_proba_ET,np.hstack((res_proba_RF,\\\n",
    "            np.hstack((res_proba_LR,np.array(train.price).reshape([res_proba_LR.shape[0],1])))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X_data).to_pickle('/home/dac/nas_storage/projects/houses/RF_25/X_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_data = pd.read_pickle('/home/dac/nas_storage/projects/houses/RF_25/X_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_data = np.array(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [2:21:49<00:00, 1704.79s/it]\n"
     ]
    }
   ],
   "source": [
    "stack_model = xgb.XGBClassifier(learning_rate=0.1,max_depth=5,n_estimators=100)\n",
    "\n",
    "res_ = np.zeros([train.shape[0],1])\n",
    "for train_fold,test_fold in tqdm(models['cross_val']):\n",
    "    stack_model.fit(\\\n",
    "            X_data[train_fold,:],train.category_id[train_fold])\n",
    "    res_[test_fold,0] = stack_model.predict(X_data[test_fold,:])\n",
    "res = result_for_avito(models['cross_val'],train.category_id, res_.reshape(1,res_.shape[0])[0],'stacked')\n",
    "models = add_models(models,stack_model,res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [2:20:47<00:00, 1685.48s/it]\n"
     ]
    }
   ],
   "source": [
    "stack_model = xgb.XGBClassifier(learning_rate=0.5,max_depth=5,n_estimators=100)\n",
    "\n",
    "res_ = np.zeros([train.shape[0],1])\n",
    "for train_fold,test_fold in tqdm(models['cross_val']):\n",
    "    stack_model.fit(\\\n",
    "            X_data[train_fold,:],train.category_id[train_fold])\n",
    "    res_[test_fold,0] = stack_model.predict(X_data[test_fold,:])\n",
    "res = result_for_avito(models['cross_val'],train.category_id, res_.reshape(1,res_.shape[0])[0],'stacked')\n",
    "models = add_models(models,stack_model,res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.954816568176 \t 0.927881860242 \t 0.854013201358 \t 0.84946791478 \t 100 \t 0.1 \t stacked\n",
      "0.894498077923 \t 0.848622368508 \t 0.754475871916 \t 0.748843579488 \t 100 \t 0.5 \t stacked\n"
     ]
    }
   ],
   "source": [
    "for i in sorted({i:models[i] for i in {k:models[k] for k in models if (k != 'cross_val')} \\\n",
    "               if (type(models[i]['model']) == type(xgb.XGBClassifier()))},\\\n",
    "              key = lambda x: models[x]['result']['4_level']['score'],reverse=True):\n",
    "    print(models[i]['result']['1_level']['score'],'\\t',\\\n",
    "          models[i]['result']['2_level']['score'],'\\t',\\\n",
    "          models[i]['result']['3_level']['score'],'\\t',\\\n",
    "          models[i]['result']['4_level']['score'],'\\t',models[i]['model'].get_params()['n_estimators'],'\\t',\\\n",
    "          models[i]['model'].get_params()['learning_rate'],'\\t',models[i]['result']['type_of_fea'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Как мы видим, использование стэка на вероятностях лучших моделей помогло на кросс-валидации немного увеличить скор\n",
    "\n",
    "> Для построения финального прогноза нами будет использоваться такая модель:\n",
    "- Логистическая регрессия с коэф регуляризации С =  3.9 и обучением свободного члена (на tf-idf матрице)\n",
    "- Случайный лес с количеством деревьев 200 и с  их глубиной 55 (на разложенной на 100 компонентов tf-idf матрице с добавлением цены лота)\n",
    "- ExtraTrees классификатор с количеством деревьев 200 и их глубиной 75 (на разложенной на 100 компонентов tf-idf матрице с добавлением цены лота)\n",
    "\n",
    "> Над предсказанием вероятности по этим трем моделям будет строиться классификатор на основе градиентного бустинга:\n",
    "- c learning_rate = 0.1, числом деревьев внутри 100 и максимальной глубиной этих деревьев 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Данная модель на пятифолдовой кросс-валидации оеспечивает следующее значение accuracy:\n",
    "\n",
    ">> На самом подробном уровне: 84,95%\n",
    "\n",
    ">> На следующем уровне:  85,40%\n",
    "\n",
    ">> На следующем уровне: 92,79%\n",
    "\n",
    ">> На уровне c самым крупным разбиением: 95,48%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построим выбранную модель и сделаем предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Next!"
     ]
    }
   ],
   "source": [
    "LR = lin.LogisticRegression(C=3.9,fit_intercept = True,max_iter = 100,n_jobs = -1,verbose=1)\n",
    "LR.fit(X_tfidf_tr,train.category_id)\n",
    "sys.stderr.write('Next!')\n",
    "f = open('LR','wb')\n",
    "pkl.dump(LR,f)\n",
    "f.close()\n",
    "\n",
    "f = open('LR','rb')\n",
    "LR = pkl.load(f)\n",
    "f.close()\n",
    "\n",
    "RF = ens.RandomForestClassifier(max_depth = 55, n_estimators=200,n_jobs = -1,verbose=2)\n",
    "RF.fit(np.hstack((X_svd_tr,np.array(train.price).reshape([train.shape[0],1])))  ,train.category_id)\n",
    "sys.stderr.write('Next!')\n",
    "\n",
    "ET = ens.ExtraTreesClassifier(n_estimators=200,max_depth=75,n_jobs=-1,verbose=2)\n",
    "ET.fit(np.hstack((X_svd_tr,np.array(train.price).reshape([train.shape[0],1])))  ,train.category_id)\n",
    "sys.stderr.write('Next!')\n",
    "\n",
    "LR_test = LR.predict_proba(X_tfidf_te)\n",
    "RF_test = RF.predict_proba(np.hstack((X_svd_te,np.array(test.price).reshape([test.shape[0],1]))))\n",
    "ET_test = ET.predict_proba(np.hstack((X_svd_te,np.array(test.price).reshape([test.shape[0],1]))))\n",
    "sys.stderr.write('Next!')\n",
    "\n",
    "X_data_test = np.hstack((ET_test,np.hstack((RF_test,\\\n",
    "            np.hstack((LR_test,np.array(test.price).reshape([LR_test.shape[0],1])))))))\n",
    "sys.stderr.write('Next!')\n",
    "\n",
    "stack_model = xgb.XGBClassifier(learning_rate=0.1,max_depth=8,n_estimators=100)\n",
    "stack_model.fit(X_data,train.category_id)\n",
    "sys.stderr.write('Next!')\n",
    "\n",
    "predicted_values = stack_model.predict(X_data_test)\n",
    "sys.stderr.write('Next!')\n",
    "\n",
    "result = pd.DataFrame()\n",
    "result['item_id'] = test.item_id\n",
    "result['category_id'] = predicted_values\n",
    "\n",
    "result.to_csv('test_category.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
